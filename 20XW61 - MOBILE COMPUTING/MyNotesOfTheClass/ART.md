# ART

## Some definitions:

### WindowManager

The WindowManager is the system service that manages the placement and z-order of windows in the window manager(?).

### SurfaceFlinger

SurfaceFlinger is the system service that manages the composition of the display. It is responsible for compositing the contents of the windows managed by the WindowManager.

**What is Composition?**

Composition is the process of combining multiple images into a single image. In the case of SurfaceFlinger, the images are the contents of the windows managed by the WindowManager.

DisplayList is a list of drawing commands that can be replayed to draw a scene.

WindowManager provides SurfaceFlinger with a list of windows to be composited. SurfaceFlinger then composites the windows into a single image and sends the image to the display.

### Rendering and Rasterization in Android

**Rendering** is a broad term that generally means transforming computer-readable information, for example objects in a 3D scene, to one or more images.

**Rasterization** is the process of converting vector data to image data. It typically means the process of transforming a vector (curve-based) image to a rasterized (pixel-based) image.

**Terms to know here:** SKIA, OpenGLES, Swift Shader Vulkan

### ActivityManager

The ActivityManager is the system service that manages the lifecycle of activities. It is responsible for starting activities, delivering intents to them, and scheduling them on to processes. It is the entry point for any application.

## Compilation

### Key terms

- hotpath - code that is executed frequently and is expensive to interpret.
- PHO - Profile Headed Optimization? Profile guided optimization? (https://en.wikipedia.org/wiki/Profile-guided_optimization)

### APK as

- **.dex** files contain Dalvik bytecode, which is the format used by the Dalvik virtual machine.
- **.odex** file is an optimized version of a **.dex** file that is execution-ready for the Dalvik virtual machine.
- **.oat** files are generated by dex2oat and contain native code generated from **.dex** files. They are used by the Android Runtime (ART) to improve performance.
- **.elf** files are Executable and Linkable Format files, which are a common standard file format for executables, object code, shared libraries, and core dumps.

**About dex2oat:**
dex2oat is a tool used by the Android Runtime (ART) to convert **.dex** files into **.oat** files. It takes an APK file and generates one or more compilation artifact files that the runtime loads. The **.oat** files contain Ahead-Of-Time (AOT) compiled code for methods in the APK. **_This improves performance by allowing the code to be executed directly, rather than being interpreted by the virtual machine._**

### Virtual Machine - Stack based vs Register based

| Feature         | Stack-based VM                                | Register-based VM                                       |
| --------------- | --------------------------------------------- | ------------------------------------------------------- |
| Examples        | Java Virtual Machine, .NET CLR                | Lua VM, Dalvik VM                                       |
| Operand storage | Stack data structure                          | Registers of the CPU                                    |
| Operand access  | Implicitly addressed by the stack pointer     | Explicitly addressed                                    |
| Code generation | Easier to generate code for a stack-based VM  | More difficult to generate code for a register-based VM |
| Efficiency      | Less efficient due to PUSH and POP operations | More efficient due to direct access to operands         |

### 3 types of compilation

- **Native** - Compiling code to machine code for a specific processor architecture. This is the most efficient way to run code, but it is also the most difficult to do. It requires a lot of work to write code for different architectures.

- **AOT** - Compiling code to machine code at compile time. This is a compromise between native code and interpreted code. It is more efficient than interpreted code, but it is not as efficient as native code. It is also more portable than native code.

- **JIT** - Compiling code to machine code at run time. This is the least efficient way to run code, but it is also the easiest to do. It is also the most portable way to run code.

| Compilation Type    | Description                                                                                                                                                                                                                          |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Native              | Native code is compiled directly for the target architecture and does not require a virtual machine to run.                                                                                                                          |
| Ahead of Time (AOT) | AOT compilation involves compiling bytecode into native code at build time, rather than at runtime. This can improve performance by allowing the code to be executed directly, rather than being interpreted by the virtual machine. |
| Just in Time (JIT)  | JIT compilation involves compiling bytecode into native code at runtime. This can improve performance by allowing frequently executed code to be compiled and optimized on-the-fly.                                                  |

### Sir's comparison of the 3 types of compilation

| Feature            | Native    | AOT         | JIT                                       |
| ------------------ | --------- | ----------- | ----------------------------------------- |
| when is it done?   | Buildtime | Installtime | Runtime                                   |
| E.g.               | GCC       | LLVM, Clang | SwiftShader                               |
| Size               | Small     | 2x          | 1.5x                                      |
| Speed              | Fast      | Medium      | Medium/Slow                               |
| Does it optimize?  | Yes       | No          | Yes (has a threshold, and optimizes then) |
| Power              | Opt       | Opt         | Not Opt                                   |
| Garbage Collection | -         | Threaded    | Main/Threaded                             |

### Additional points to note

- JIT is slow on the first load due to the steps it takes to compile the code. (Load -> Init)
- AOT is natively compiled, so it has slow startup time.
- JIT has an interpreter, and if the threshold is crossed, standard C1 and optimizing C2 compiler, and it loads the bytecode. All this is too much for an Embedded device.
- The optimizations by C2 include **code sinking** (different assembly instructions for null pointers) and **code layout** (puts important startup code at top for fast readahead).

### The process of compilation

1. `.kt` or `.java` files are compiled by `javac` into `.class` files.
2. `.class` files are then converted by `v8` or `dx` into `.dex` files.
3. `.dex` files are packaged into an `apk`.
4. At runtime, the `dex2oat` tool converts the `apk` into an `.oat` file.
5. The `.oat` file is then executed.
6. During execution, the code can be further optimized and saved as a combination of `.oat` and `.dex` files.
7. The execution can help improve the `PHO` file of the app. (Profile Headed Optimization?)

By combining the use of `.oat` files and the `JIT cache`, the Android Runtime (ART) can continually profile and optimize the performance of an application over time. This means that as the application is used, its performance should improve as frequently executed code is optimized and stored in the JIT cache.

`boot_class_path` is a list of paths to the core libraries that are always loaded by the Android Runtime (ART). These libraries are stored in the /system/framework directory.

    boot_class_path -> code layout -> [OAT | DEX] in a file.

As the app is used, the OAT% increases, and the DEX% decreases. There is a threshold of 20%.

## Allocation and Garbage Collection

### Interesting TLB issue

The TLB (Translation Lookaside Buffer) is a cache used by the MMU (Memory Management Unit) to speed up virtual-to-physical address translations. When a virtual memory address is accessed, the MMU checks the TLB to see if it has a cached translation for that address. If it does, the translation can be performed quickly without accessing the page table.

However, if the memory mapping for a virtual address changes (for example, if the memory is freed and then reallocated), the TLB may still contain the old translation. This can result in the CPU reading from or writing to an incorrect physical memory location.

Here is an example that demonstrates this issue:

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

int main() {
    char *ptr1 = malloc(100);
    strcpy(ptr1, "Hello World!");
    printf("%s\n", ptr1);

    free(ptr1);

    char *ptr2 = malloc(100);
    printf("%s\n", ptr2);

    return 0;
}
```

In this example, we allocate a block of memory using `malloc` and store a string in it. We then free the memory and allocate another block of the same size. If the TLB still contains the old translation for the virtual address of `ptr1`, then `ptr2` may point to the same physical memory location as `ptr1`. This means that when we print the value of `ptr2`, it may still contain the string "Hello World!" even though we never explicitly stored it there.

To prevent this issue, the operating system must ensure that the TLB is properly invalidated when memory mappings change. This can be done using various techniques such as flushing the entire TLB or selectively invalidating individual entries.

**All the processes are subject to the MMU, but some may escape this thru the IOMMU. IOMMU makes it janky, so we go with DMA.
E.g. GPU does this for displaying- 4k pixels, 60 times per second.**

### Heap and Fragmentation

- Heap growth: increasing heap memory size for dynamic memory allocation.
- Fragmentation: memory broken into small, non-contiguous blocks. Two types: internal and external.
  - Internal fragmentation: fixed-size blocks allocated to processes requiring less memory. Difference between block size and required memory is wasted.
  - External fragmentation: enough total free memory to satisfy request, but not contiguous. Cannot be used because not in single contiguous block.
- Compaction: technique to reduce external fragmentation by rearranging memory blocks to create larger contiguous blocks of free memory. Can be expensive and temporarily disrupt normal operation.

In summary, internal fragmentation refers to unused space within allocated memory blocks, while external fragmentation refers to unused space between allocated memory blocks. Compaction can help reduce external fragmentation but can be an expensive operation.

### Thread Local Allocation Buffers (TLABs) and Bump Pointer Allocation (BPA)

- Thread Local Allocation Buffers (TLABs) are regions of memory inside the heap that are exclusively assigned to individual threads. Each thread has its own TLAB, which it can use to allocate new objects without the need for synchronization with other threads.

- Bump Pointer Allocation (BPA) is a fast memory allocation technique used within TLABs. When a thread needs to allocate memory within its TLAB, it simply increments a pointer (the “bump pointer”) to the next free memory location. This allows for fast allocation of new objects without the need for complex data structures or synchronization.

Bump Pointer Allocation (BPA) requires a contiguous chunk of memory to allocate to, which brings back the need for heap compaction. This is why this allocation mechanism is sometimes called “pointer bump allocation”

### Garbage Collection

Garbage collection is a form of automatic memory management. The garbage collector is responsible for identifying and reclaiming memory that is no longer in use by the program.

There are many different garbage collection algorithms, like:

- **Stop-the-world:** Pause the execution of the application threads to perform garbage collection.

- **Concurrent Mark and Sweep:** Mark the objects that are not in use concurrently with the application threads running, and then pause the application threads to sweep them.

- **Generational GC:** Segregate the objects into generations based on their age, and then perform garbage collection on the younger generations more frequently than on the older generations.

- **Compacting GC:** Compact the heap to reduce fragmentation and then perform garbage collection.

### More random info in notes

- ART optimization > Dalvik optimization
- ART supports heap extension, Dalvik does not.

### Green Threads

Green threads are user-level threads that are scheduled by a runtime library or virtual machine (VM) instead of the operating system. They are called “green” because they are lightweight and require less resources than native threads. Green threads can be implemented using an N:M threading model, where N green threads are multiplexed onto M kernel threads.

Threads, on the other hand, are scheduled by the operating system and can be implemented using a 1:1 threading model, where each user-level thread is mapped to a kernel thread 1.

- E.g. Go routines and Coroutines.

### Road so far of the GC

1. DVM to JIT to ART(AOT)
2. JIT + AOT
3. CGC (Generational GC)
4. Concurrent GC

## The Garbage Collection Algorithms

### Concurrent Mark and Sweep

**The tri-color scheme to know about:**
Tri-color marking as a garbage collection mechanism that walks the heap and assigns each object a color: `white`, `grey`, or `black`.

- White objects are no longer referenced and will be freed.
- Grey objects have pointers to them but have not been scanned for pointers to other objects.
- Black objects have been discovered and contain no pointers into white objects.

The tri-color invariant states that no objects in the black set reference objects in the white set. The process starts with all roots in the grey set, all other objects in the white set, and an empty black set. The process continues until the grey set is empty. At the end of the marking process, the black set contains all objects to keep and the white set contains objects to release.

1. **Initial Mark:** Identify the root set (stop-the-world).

2. **Concurrent Mark:** Trace the object graph starting from the root set to identify all live objects (concurrent).

3. **Preclean/Final Remark:** Account for any updates to the object graph that occurred during the Concurrent Mark phase and mark any missed live objects (stop-the-world).

4. **Sweep:** Reclaim memory occupied by dead objects (concurrent).

### Generational Garbage Collection

Generational garbage collection divides the heap into two or more generations: a young generation and one or more old generations.

- **Young Generation:** Contains newly allocated objects. When the young generation fills up, a minor garbage collection is triggered to reclaim memory occupied by dead objects in the young generation. Live objects that survive a minor garbage collection are promoted to an old generation.
- **Old Generation:** Contains objects that have survived multiple minor garbage collections. When an old generation fills up, a major garbage collection is triggered to reclaim memory occupied by dead objects in both the young and old generations.

### Compacting Garbage Collection

Compacting garbage collection moves live objects in the heap to eliminate fragmentation and create large contiguous blocks of free memory.

1. **Mark:** Identify all the live objects in the heap by tracing the object graph starting from the root set.
2. **Compact:** Move live objects to eliminate fragmentation and create large contiguous blocks of free memory. Update all references to moved objects to reflect their new locations.
3. **Sweep:** Reclaim memory occupied by dead objects by sweeping through the heap and freeing memory occupied by objects that were not marked as live during the Mark phase.

### G1 Garbage Collector

G1GC (Garbage First Garbage Collector) is a garbage collector used by the Java Virtual Machine (JVM). It divides the heap into multiple regions and performs garbage collection on a set of regions with the most reclaimable space first.

In G1GC, the heap is divided into several logical areas: Eden, Survivor (S1 and S2), and Old.

- **Eden:** Contains newly allocated objects.
- **Survivor (S1 and S2):** Contains objects that have survived at least one garbage collection cycle in the Eden space. There are two survivor spaces (S1 and S2) that are used alternately. One survivor space is always empty.
- **Old:** Contains objects that have survived multiple garbage collection cycles in the Eden and Survivor spaces.

### Semi-Space Garbage Collection/Copy Collector with Cheney’s Algorithm

Semi-space garbage collection divides the heap into two equally sized semi-spaces: a “from” space and a “to” space.

- **Copy:** Identify all the live objects in the “from” space by tracing the object graph starting from the root set. Copy all live objects from the “from” space to the “to” space using Cheney’s Algorithm. Update all references to copied objects to reflect their new locations in the “to” space.

- **Swap:** Swap the roles of the “from” and “to” spaces. The “to” space becomes the new “from” space and vice versa.

The garbage collection happens implicitly during the `Copy` phase. When live objects are copied from the “from” space to the “to” space using Cheney’s Algorithm, any objects that are not copied are considered garbage and their memory is implicitly reclaimed when the roles of the “from” and “to” spaces are swapped. The entire “from” space, which now contains only garbage objects, becomes free memory that can be used for new allocations.

### Some more stuff related to the GC Algorithms

- **Rosalloc:** A thread-aware memory allocator used by ART that allows for efficient allocation and reduces the need for stop-the-world garbage collection events.

  - Uses per thread locks. Allocator cannot allocate if GC holds the lock.

- ART only does compaction when app runs in background. Space not enough for allocation when app in foreground? stop app and do compaction.

- **Weak Gen Hypothesis:** The weak generational hypothesis states that most objects die young. This hypothesis is used to justify the use of generational garbage collection.

- BPA is 18x faster than Rosalloc.

## Four types of memory

In the context of virtual memory management, memory can be classified based on its content (clean or dirty) and its accessibility (private or shared).

- **Clean memory:** Memory whose content has not been modified since it was loaded from storage (e.g., disk) into main memory (e.g., RAM).
- **Dirty memory:** Memory whose content has been modified since it was loaded from storage into main memory.
- **Shared memory:** Memory that can be accessed by multiple processes.
- **Private memory:** Memory that can only be accessed by a single process.

Here are some examples of the different combinations of these types of memory:

- **Clean + Private:** dex files. (Can be paged out to clean-shared memory)
- **Dirty + Private:** Any objects created by a process.
- **Dirty + Shared:** Zygote (copy-on-write, when written to by a process, it is becomes dirty + private)
- **Clean + Shared:** DLL (Dynamic Link Library) files.

Dirty + private and Dirty + Shared are expensive.

### Other things happening in memory

**A lil bit of mmap'ing:** `mmap` is a system call that maps files or devices into memory. It can be used to map a file or a portion of a file into the address space of a process. This allows the process to access the file’s contents as if it were in memory, which can improve performance by reducing the number of disk accesses.

If the code segment of SQLite is `mmap`’ed into the address spaces of processes `Pa` and `Pb`, it means that both processes can access the code segment of `SQLite` as if it were in their own memory. This allows both processes to execute the code in the `SQLite` code segment without having to load it from disk into their own memory.

**A lil bit about memory barrier ABI construct:** A memory barrier is a low-level instruction that **_enforces ordering_** of memory operations. It’s used in code that operates on shared memory, such as synchronization primitives and lock-free data structures on multiprocessor systems.

**Caches and spilling:** A CPU (Central Processing Unit) is the primary component of a computer that performs most of the processing. Data is stored in several caches (L1, L2, L3) and RAM (Random Access Memory) to optimize processing speed. The closer a cache is to the CPU in terms of levels (L1 being the closest), the smaller and faster it is. When data is not found in one cache level, the next level is checked until it is found or until RAM is reached.

Spilling refers to when registers are full and some data must be moved to memory to free up space. This can happen when there are more live variables than registers available. When this happens, some data must be moved from registers to memory (usually RAM) to free up space for new data. This process can slow down processing as accessing memory is slower than accessing registers.

**What to do when there is going to be a data race?**

- A **data race** occurs when multiple threads access shared data concurrently.
- To prevent this, techniques such as **locks**, **atomics**, and the **volatile** keyword in Java can be used.
  - **Locks** synchronize access to shared data.
  - **Atomics** perform atomic operations without locks.
  - The **volatile** keyword prevents local caching of a variable.

---

We are leaving a bit of content out here -- It covers, local block, fn block, global block. Memory barrier before Android 12. userfaultd.

---

## More

- GC + Allocator (Rosalloc)
- Interpreting (DVM)
- Compiling, Interpreting (ART)

There is this Register allocation problem. It is a NP-hard problem.

- HLL -> AST + CFG -> IR -> 3AC + Inf. registers(allocated to finite registers) in Machine code

IR (Inf.) -> CPU (Finite) + RAM (Spill)

**How did Android tackle this problem?**

Ian Rogers worked on SSA-based register allocation, which takes advantage of the properties of SSA form to perform register allocation more efficiently.

One approach is Linear Scan Register Allocation on SSA Form, which simplifies the linear scan algorithm by taking advantage of the structural properties of SSA form.

### Linear Scan Register Allocation

Linear Scan Register Allocation is an algorithm for assigning registers to variables in a program. It works by scanning the variables in the program in a linear order and assigning registers to them as they become live. When all registers are in use and a new variable becomes live, the algorithm selects a variable to spill to memory to free up a register.

Here is an example of how Linear Scan Register Allocation might work on a simple Three-Address Code (3AC) program:

```
a = 1
b = 2
c = a + b
d = c * 3
```

Let’s assume that we have two registers available, `R1` and `R2`.

1. The first instruction assigns the value 1 to variable `a`. Since `a` is now live and there is a free register available, the algorithm assigns `R1` to `a`.
2. The second instruction assigns the value 2 to variable `b`. Since `b` is now live and there is still a free register available, the algorithm assigns `R2` to `b`.
3. The third instruction computes the sum of `a` and `b` and assigns the result to variable `c`. Since both `a` and `b` are already in registers, the algorithm can use their values directly. After this instruction, variable `a` is no longer live, so its register can be reused. The algorithm assigns `R1` to `c`.
4. The fourth instruction computes the product of `c` and `3` and assigns the result to variable `d`. Since `c` is already in a register, the algorithm can use its value directly. After this instruction, variable `c` is no longer live, so its register can be reused. The algorithm assigns `R1` to `d`.

In this example, all variables were successfully assigned to registers without the need for spilling.

    Try to find an example where spilling happens.
